"use client";

import { useCallback, useEffect, useMemo, useRef, useState } from "react";
import { getAddressFromCoordinates } from "@/services/geocoding";
import { GoogleGenerativeAI } from "@google/generative-ai";
import clsx from "clsx";

// å®šç¾© Web Speech API é¡å‹
interface SpeechRecognitionEvent extends Event {
  results: {
    [index: number]: {
      [index: number]: {
        transcript: string;
      };
    };
  };
}

interface SpeechRecognition extends EventTarget {
  continuous: boolean;
  interimResults: boolean;
  lang: string;
  onresult: ((event: SpeechRecognitionEvent) => void) | null;
  onend: (() => void) | null;
  start: () => void;
  stop: () => void;
}

interface SpeechRecognitionConstructor {
  new (): SpeechRecognition;
}

declare global {
  interface Window {
    SpeechRecognition: SpeechRecognitionConstructor;
    webkitSpeechRecognition: SpeechRecognitionConstructor;
  }
}

const CameraComponent = () => {
  const videoRef = useRef<HTMLVideoElement>(null);
  const [description, setDescription] = useState<string>("");
  const [isAnalyzing, setIsAnalyzing] = useState(false);
  const [error, setError] = useState<string>("");
  const [isListening, setIsListening] = useState(false);
  const [userQuestion, setUserQuestion] = useState<string>("");
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [location, setLocation] = useState<{
    latitude: number;
    longitude: number;
  } | null>(null);

  const genAI = useMemo(
    () => new GoogleGenerativeAI(process.env.NEXT_PUBLIC_GEMINI_API_KEY || ""),
    []
  );
  const recognitionRef = useRef<SpeechRecognition | null>(null);

  // èªéŸ³æ’­æ”¾åŠŸèƒ½
  const speak = useCallback((text: string) => {
    if ("speechSynthesis" in window) {
      setIsSpeaking(true);
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = "zh-TW";
      utterance.onend = () => setIsSpeaking(false);
      speechSynthesis.speak(utterance);
    }
  }, []);

  // ç²å–åœ°å€
  const getAddress = useCallback(
    async (latitude: number, longitude: number) => {
      return getAddressFromCoordinates(latitude, longitude);
    },
    []
  );

  // è™•ç†ä½¿ç”¨è€…å•é¡Œ
  const handleUserQuestion = useCallback(
    async (question: string) => {
      if (!videoRef.current || !location) return;

      try {
        setIsAnalyzing(true);
        const model = genAI.getGenerativeModel({
          model: "gemini-2.0-flash-lite",
        });

        const canvas = document.createElement("canvas");
        canvas.width = videoRef.current.videoWidth;
        canvas.height = videoRef.current.videoHeight;
        const ctx = canvas.getContext("2d");

        if (!ctx) throw new Error("ç„¡æ³•å‰µå»º canvas context");

        ctx.drawImage(videoRef.current, 0, 0);
        const imageData = canvas.toDataURL("image/jpeg");

        // åœ¨é€™è£¡ç²å–åœ°å€
        const address = await getAddress(location.latitude, location.longitude);
        const locationInfo = address
          ? `ç›®å‰ä½ç½®ï¼š${address}`
          : `ç›®å‰ä½ç½®ï¼šç·¯åº¦ ${location.latitude}ï¼Œç¶“åº¦ ${location.longitude}`;

        const prompt = `ä»¥ä¸‹æ˜¯ä½¿ç”¨è€…çš„å•é¡Œï¼š${question}\n${locationInfo}\nè«‹æ ¹æ“šç…§ç‰‡å…§å®¹å’Œä½ç½®è³‡è¨Šï¼Œç”¨ç¹é«”ä¸­æ–‡å›ç­”é€™å€‹å•é¡Œã€‚å¦‚æœå•é¡Œèˆ‡ç…§ç‰‡å…§å®¹ç„¡é—œï¼Œè«‹å‘ŠçŸ¥ä½¿ç”¨è€…ã€‚å›ç­”è¦ç°¡æ½”æœ‰åŠ›ã€‚`;

        const result = await model.generateContent([
          prompt,
          {
            inlineData: {
              mimeType: "image/jpeg",
              data: imageData.split(",")[1],
            },
          },
        ]);

        const response = await result.response;
        const aiResponse = response.text();
        setDescription(aiResponse);
        speak(aiResponse);
      } catch (error) {
        console.error("è™•ç†å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤:", error);
        setDescription("è™•ç†å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹å†è©¦ä¸€æ¬¡ã€‚");
      } finally {
        setIsAnalyzing(false);
      }
    },
    [genAI, speak, location, getAddress]
  );

  // åˆå§‹åŒ–èªéŸ³è­˜åˆ¥
  useEffect(() => {
    if ("SpeechRecognition" in window || "webkitSpeechRecognition" in window) {
      const SpeechRecognition =
        window.SpeechRecognition || window.webkitSpeechRecognition;
      recognitionRef.current = new SpeechRecognition();
      recognitionRef.current.continuous = false;
      recognitionRef.current.interimResults = false;
      recognitionRef.current.lang = "zh-TW";

      const handleSpeechResult = (event: SpeechRecognitionEvent) => {
        const transcript = event.results[0][0].transcript;
        setUserQuestion(transcript);
        handleUserQuestion(transcript);
      };

      recognitionRef.current.onresult = handleSpeechResult;
      recognitionRef.current.onend = () => {
        setIsListening(false);
      };
    } else {
      setError("æ‚¨çš„ç€è¦½å™¨ä¸æ”¯æ´èªéŸ³è­˜åˆ¥åŠŸèƒ½");
    }

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.onresult = null;
        recognitionRef.current.onend = null;
      }
    };
  }, [handleUserQuestion]);

  // é–‹å§‹èªéŸ³è¼¸å…¥
  const startListening = useCallback(() => {
    if (recognitionRef.current && !isListening) {
      recognitionRef.current.start();
      setIsListening(true);
    }
  }, [isListening]);

  // ç²å–åœ°ç†ä½ç½®
  useEffect(() => {
    if ("geolocation" in navigator) {
      navigator.geolocation.getCurrentPosition(
        (position) => {
          const { latitude, longitude } = position.coords;
          setLocation({ latitude, longitude });
        },
        (error) => {
          console.error("ç„¡æ³•ç²å–ä½ç½®:", error);
        }
      );
    }
  }, []);

  useEffect(() => {
    const startCamera = async () => {
      try {
        // é¦–å…ˆå˜—è©¦ä½¿ç”¨å¾Œç½®ç›¸æ©Ÿ
        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            video: { facingMode: { exact: "environment" } },
          });
          if (videoRef.current) {
            videoRef.current.srcObject = stream;
          }
          return;
        } catch {
          console.log("ç„¡æ³•ä½¿ç”¨å¾Œç½®ç›¸æ©Ÿï¼Œå˜—è©¦å…¶ä»–ç›¸æ©Ÿé¸é …");
        }

        // å¦‚æœç„¡æ³•ä½¿ç”¨å¾Œç½®ç›¸æ©Ÿï¼Œå˜—è©¦ä½¿ç”¨ä»»ä½•å¯ç”¨çš„ç›¸æ©Ÿ
        const stream = await navigator.mediaDevices.getUserMedia({
          video: true,
        });
        if (videoRef.current) {
          videoRef.current.srcObject = stream;
        }
        setError(""); // æ¸…é™¤éŒ¯èª¤è¨Šæ¯
      } catch (err) {
        console.error("ç„¡æ³•å­˜å–ä»»ä½•ç›¸æ©Ÿ:", err);
        setError("ç„¡æ³•å­˜å–ç›¸æ©Ÿï¼Œè«‹ç¢ºèªç›¸æ©Ÿæ¬Šé™å·²é–‹å•Ÿ");
      }
    };

    startCamera();

    const video = videoRef.current;

    return () => {
      if (video?.srcObject) {
        const tracks = (video.srcObject as MediaStream).getTracks();
        tracks.forEach((track) => track.stop());
      }
      // æ¸…ç†èªéŸ³åˆæˆ
      if ("speechSynthesis" in window) {
        speechSynthesis.cancel();
      }
    };
  }, []);

  return (
    <div className="h-screen w-screen relative overflow-hidden">
      <video
        ref={videoRef}
        autoPlay
        playsInline
        className="w-full h-full object-cover absolute top-0 left-0 z-10"
      />

      {/* éŒ¯èª¤æç¤º */}
      {error && (
        <div className="absolute top-5 left-1/2 -translate-x-1/2 px-6 py-3 bg-red-500/80 text-white rounded-lg z-50">
          {error}
        </div>
      )}

      {/* AI å›æ‡‰ */}
      {description && (
        <div className="absolute top-5 left-1/2 -translate-x-1/2 w-[90%] max-w-2xl p-4 bg-black/70 text-white rounded-xl z-50 animate-fadeIn">
          {isSpeaking && (
            <div className="text-green-400 mb-2">ğŸ”Š æ­£åœ¨ç‚ºæ‚¨è§£èªª...</div>
          )}
          <div className="leading-relaxed">{description}</div>
        </div>
      )}

      {/* éº¥å…‹é¢¨æŒ‰éˆ•å’Œä½¿ç”¨è€…å•é¡Œ */}
      <div className="absolute bottom-10 left-1/2 -translate-x-1/2 flex flex-col items-center gap-4 z-50">
        {userQuestion && (
          <div className="px-4 py-2 bg-white/90 rounded-full text-sm text-black max-w-[80vw] text-center">
            {userQuestion}
          </div>
        )}
        <button
          onClick={startListening}
          disabled={isListening || isAnalyzing}
          className={clsx(
            "w-16 h-16 rounded-full flex items-center justify-center text-2xl shadow-lg transition-all duration-300",
            isListening
              ? "bg-red-500 cursor-not-allowed"
              : "bg-green-500 hover:scale-105 active:scale-95"
          )}
        >
          {isListening ? "ğŸ¤" : "ğŸ™ï¸"}
        </button>
        {isAnalyzing && (
          <div className="text-white bg-black/50 px-4 py-2 rounded-full text-sm">
            åˆ†æä¸­...
          </div>
        )}
      </div>
    </div>
  );
};

export default CameraComponent;
